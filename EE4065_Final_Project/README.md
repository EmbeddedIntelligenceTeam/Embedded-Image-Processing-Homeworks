# EE4065 Embedded Digital Image Processing - Final Project

<p align="center">
  <img src="https://img.shields.io/badge/Platform-ESP32--CAM-blue" alt="Platform">
  <img src="https://img.shields.io/badge/Framework-Arduino-00979D" alt="Framework">
  <img src="https://img.shields.io/badge/ML-TensorFlow Lite-FF6F00" alt="TensorFlow Lite">
  <img src="https://img.shields.io/badge/Language-Python%20%7C%20C%2B%2B-green" alt="Languages">
</p>

> **Yeditepe Ãœniversitesi - Elektrik-Elektronik MÃ¼hendisliÄŸi BÃ¶lÃ¼mÃ¼**  
> **GÃ¶mÃ¼lÃ¼ Dijital GÃ¶rÃ¼ntÃ¼ Ä°ÅŸleme Final Projesi**

Bu proje, ESP32-CAM modÃ¼lÃ¼ Ã¼zerinde Ã§eÅŸitli gÃ¶rÃ¼ntÃ¼ iÅŸleme ve makine Ã¶ÄŸrenimi tekniklerini uygulayarak el yazÄ±sÄ± rakam tanÄ±ma ve tespit sistemleri geliÅŸtirmeyi amaÃ§lamaktadÄ±r.

---

## ğŸ“‹ Ä°Ã§indekiler

- [Proje Genel BakÄ±ÅŸ](#-proje-genel-bakÄ±ÅŸ)
- [DonanÄ±m Gereksinimleri](#-donanÄ±m-gereksinimleri)
- [YazÄ±lÄ±m Gereksinimleri](#-yazÄ±lÄ±m-gereksinimleri)
- [Proje YapÄ±sÄ±](#-proje-yapÄ±sÄ±)
- [Soru 1: Thresholding (EÅŸikleme)](#-soru-1-thresholding-eÅŸikleme)
- [Soru 2: YOLO ile Rakam Tespiti](#-soru-2-yolo-ile-rakam-tespiti)
- [Soru 3: Upsampling ve Downsampling](#-soru-3-upsampling-ve-downsampling)
- [Soru 4: Multi-Model Rakam TanÄ±ma](#-soru-4-multi-model-rakam-tanÄ±ma)
- [Soru 5: FOMO ile Rakam Tespiti (Bonus)](#-soru-5-fomo-ile-rakam-tespiti-bonus)
- [Kurulum ve Ã‡alÄ±ÅŸtÄ±rma](#-kurulum-ve-Ã§alÄ±ÅŸtÄ±rma)
- [Test SonuÃ§larÄ±](#-test-sonuÃ§larÄ±)
- [Referanslar](#-referanslar)

---

## ğŸ¯ Proje Genel BakÄ±ÅŸ

Bu proje, ESP32-CAM modÃ¼lÃ¼ kullanarak gerÃ§ek zamanlÄ± gÃ¶rÃ¼ntÃ¼ iÅŸleme uygulamalarÄ± geliÅŸtirmektedir. Her soru farklÄ± bir gÃ¶rÃ¼ntÃ¼ iÅŸleme veya makine Ã¶ÄŸrenimi tekniÄŸini kapsamaktadÄ±r:

| Soru | Konu | Puan | Durum |
|------|------|------|-------|
| Q1 | Thresholding (EÅŸikleme) | 20 | âœ… TamamlandÄ± |
| Q2 | YOLO Rakam Tespiti | 40 | âœ… TamamlandÄ± |
| Q3 | Upsampling/Downsampling | 20 | âœ… TamamlandÄ± |
| Q4 | Multi-Model CNN | 20 | âœ… TamamlandÄ± |
| Q5 | FOMO Rakam Tespiti (Bonus) | 20 | âœ… TamamlandÄ± |

### KullanÄ±lan Teknolojiler

- **DonanÄ±m**: AI-Thinker ESP32-CAM (OV2640 kamera sensÃ¶rÃ¼)
- **GeliÅŸtirme OrtamÄ±**: Arduino IDE 2.x, Python 3.10+
- **ML Framework**: TensorFlow 2.x, TensorFlow Lite Micro
- **Web ArayÃ¼zÃ¼**: HTML5, CSS3, JavaScript (ESP32 Ã¼zerinde WebServer)

---

## ğŸ”§ DonanÄ±m Gereksinimleri

### Ana DonanÄ±m

| BileÅŸen | AÃ§Ä±klama |
|---------|----------|
| ESP32-CAM | AI-Thinker modÃ¼lÃ¼ (4MB Flash, PSRAM) |
| USB-TTL DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ | FTDI FT232RL veya CH340G |
| GÃ¼Ã§ KaynaÄŸÄ± | 5V, min 500mA |

### ESP32-CAM Pin BaÄŸlantÄ±larÄ± (AI-Thinker)

```
ESP32-CAM          USB-TTL
---------          -------
GND       <-->     GND
5V        <-->     5V
U0R (GPIO3) <-->   TX
U0T (GPIO1) <-->   RX
GPIO0     <-->     GND (sadece programlama sÄ±rasÄ±nda)
```

### Kamera SensÃ¶rÃ¼ Pin YapÄ±landÄ±rmasÄ±

```cpp
#define PWDN_GPIO_NUM     32    // Power Down
#define RESET_GPIO_NUM    -1    // Reset (kullanÄ±lmÄ±yor)
#define XCLK_GPIO_NUM      0    // External Clock
#define SIOD_GPIO_NUM     26    // SCCB Data
#define SIOC_GPIO_NUM     27    // SCCB Clock
#define Y9_GPIO_NUM       35    // Pixel Data Bit 9
#define Y8_GPIO_NUM       34    // Pixel Data Bit 8
#define Y7_GPIO_NUM       39    // Pixel Data Bit 7
#define Y6_GPIO_NUM       36    // Pixel Data Bit 6
#define Y5_GPIO_NUM       21    // Pixel Data Bit 5
#define Y4_GPIO_NUM       19    // Pixel Data Bit 4
#define Y3_GPIO_NUM       18    // Pixel Data Bit 3
#define Y2_GPIO_NUM        5    // Pixel Data Bit 2
#define VSYNC_GPIO_NUM    25    // Vertical Sync
#define HREF_GPIO_NUM     23    // Horizontal Reference
#define PCLK_GPIO_NUM     22    // Pixel Clock
```

---

## ğŸ’» YazÄ±lÄ±m Gereksinimleri

### Python OrtamÄ±

```bash
# Python 3.10+ gerekli
pip install tensorflow>=2.15.0
pip install numpy>=1.24.0
pip install opencv-python>=4.8.0
pip install matplotlib>=3.7.0
pip install pillow>=10.0.0
```

### Arduino IDE AyarlarÄ±

1. **Board Manager URL** (File > Preferences):
   ```
   https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json
   ```

2. **Board SeÃ§imi**: `AI Thinker ESP32-CAM`

3. **Gerekli KÃ¼tÃ¼phaneler**:
   - `TensorFlowLite_ESP32` (Library Manager'dan)
   - ESP32 Camera kÃ¼tÃ¼phanesi (ESP32 board paketi ile gelir)

4. **Upload AyarlarÄ±**:
   - Flash Mode: `QIO`
   - Flash Frequency: `80MHz`
   - Partition Scheme: `Huge APP (3MB No OTA/1MB SPIFFS)`
   - Upload Speed: `921600`

---

## ğŸ“ Proje YapÄ±sÄ±

```
EE4065_Final_Project/
â”œâ”€â”€ README.md                           # Bu dosya
â”œâ”€â”€ EE4065 Final Project.md             # Proje gereksinimleri
â”œâ”€â”€ EE4065 Final Project.pdf            # Proje gereksinimleri (PDF)
â”‚
â”œâ”€â”€ Q1_Thresholding/                    # Soru 1: EÅŸikleme
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ thresholding.py             # PC Ã¼zerinde Ã§alÄ±ÅŸan Python kodu
â”‚   â””â”€â”€ esp32_cam/
â”‚       â””â”€â”€ esp32_thresholding/
â”‚           â””â”€â”€ esp32_thresholding.ino  # ESP32-CAM Arduino kodu
â”‚
â”œâ”€â”€ Q2_YOLO_Digit_Detection/            # Soru 2: YOLO Rakam Tespiti
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ train_yolo_tiny.py          # YOLO model eÄŸitimi
â”‚   â”‚   â”œâ”€â”€ export_tflite.py            # TFLite dÃ¶nÃ¼ÅŸÃ¼mÃ¼
â”‚   â”‚   â”œâ”€â”€ test_detection.py           # Model test scripti
â”‚   â”‚   â”œâ”€â”€ yolo_tiny_digit.h5          # EÄŸitilmiÅŸ Keras modeli
â”‚   â”‚   â””â”€â”€ yolo_tiny_digit.tflite      # TFLite modeli
â”‚   â””â”€â”€ esp32_cam/
â”‚       â””â”€â”€ ESP32_YOLO_Web/
â”‚           â”œâ”€â”€ ESP32_YOLO_Web.ino      # ESP32-CAM inference kodu
â”‚           â””â”€â”€ yolo_model_data.h       # TFLite model verisi (C array)
â”‚
â”œâ”€â”€ Q3_Upsampling_Downsampling/         # Soru 3: Yeniden Ã–rnekleme
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ resampling.py               # Python implementasyonu
â”‚   â””â”€â”€ esp32_cam/
â”‚       â””â”€â”€ esp32_resampling/
â”‚           â””â”€â”€ esp32_resampling.ino    # ESP32-CAM implementasyonu
â”‚
â”œâ”€â”€ Q4_Multi_Model/                     # Soru 4: Ã‡oklu Model CNN
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ train_models.py             # Model eÄŸitim scripti
â”‚   â”‚   â””â”€â”€ export_tflite.py            # TFLite dÃ¶nÃ¼ÅŸÃ¼mÃ¼
â”‚   â””â”€â”€ esp32_cam/
â”‚       â””â”€â”€ CNN/
â”‚           â””â”€â”€ digit_recognition/
â”‚               â”œâ”€â”€ digit_recognition.ino    # ESP32-CAM inference
â”‚               â””â”€â”€ model_data.h             # TFLite model verisi
â”‚
â”œâ”€â”€ Q5_FOMO_SSD/                        # Soru 5: FOMO Rakam Tespiti
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ train_fomo.py               # FOMO model eÄŸitimi
â”‚   â”‚   â”œâ”€â”€ export_tflite.py            # TFLite dÃ¶nÃ¼ÅŸÃ¼mÃ¼
â”‚   â”‚   â”œâ”€â”€ predict.py                  # Tahmin scripti
â”‚   â”‚   â”œâ”€â”€ fomo_digit.h5               # EÄŸitilmiÅŸ model
â”‚   â”‚   â””â”€â”€ fomo_digit.tflite           # TFLite modeli
â”‚   â””â”€â”€ esp32_cam/
â”‚       â””â”€â”€ esp32_fomo_digit/
â”‚           â”œâ”€â”€ esp32_fomo_digit.ino    # ESP32-CAM inference
â”‚           â””â”€â”€ model_data.h            # TFLite model verisi
â”‚
â””â”€â”€ Q6_MobileViT/                       # Soru 6: MobileViT (Bonus - YapÄ±lmadÄ±)
```

---

## ğŸ” Soru 1: Thresholding (EÅŸikleme)

### Problem TanÄ±mÄ±

ESP32-CAM tarafÄ±ndan alÄ±nan gÃ¶rÃ¼ntÃ¼de, arka plana gÃ¶re daha parlak olan bir nesnenin tespiti yapÄ±lacaktÄ±r. Tespit edilecek nesnenin **1000 piksel** olduÄŸu bilinmektedir. Bu bilgi kullanÄ±larak boyut bazlÄ± eÅŸikleme gerÃ§ekleÅŸtirilecektir.

### Algoritma AÃ§Ä±klamasÄ±

#### 1. Histogram Analizi
GÃ¶rÃ¼ntÃ¼nÃ¼n histogram'Ä± Ã§Ä±karÄ±larak piksel yoÄŸunluk daÄŸÄ±lÄ±mÄ± analiz edilir.

#### 2. Boyut BazlÄ± EÅŸik Belirleme
Hedef nesnenin 1000 piksel olduÄŸu bilindiÄŸinden, eÅŸik deÄŸeri ÅŸu ÅŸekilde belirlenir:
- Histogram kÃ¼mÃ¼latif olarak hesaplanÄ±r
- Toplam piksel sayÄ±sÄ±ndan 1000 Ã§Ä±karÄ±larak hedef kÃ¼mÃ¼latif deÄŸer bulunur
- Bu deÄŸere karÅŸÄ±lÄ±k gelen yoÄŸunluk eÅŸik olarak kullanÄ±lÄ±r

```python
# Algoritma kodu
def find_threshold_by_object_size(image, target_size=1000):
    """
    Nesne boyutuna gÃ¶re eÅŸik deÄŸeri belirleme.
    
    Args:
        image: Grayscale gÃ¶rÃ¼ntÃ¼ (numpy array)
        target_size: Hedef nesne piksel sayÄ±sÄ±
    
    Returns:
        threshold: Hesaplanan eÅŸik deÄŸeri
    """
    hist, bins = np.histogram(image.flatten(), bins=256, range=(0, 256))
    cumsum = np.cumsum(hist[::-1])  # YÃ¼ksekten dÃ¼ÅŸÃ¼ÄŸe kÃ¼mÃ¼latif toplam
    
    # Hedef boyuta ulaÅŸan indeksi bul
    target_idx = np.searchsorted(cumsum, target_size)
    threshold = 255 - target_idx
    
    return threshold
```

### ESP32-CAM Implementasyonu

```cpp
// Boyut bazlÄ± eÅŸik hesaplama
uint8_t calculateThresholdBySize(uint8_t* image, int width, int height, int targetSize) {
    // Histogram oluÅŸtur
    int histogram[256] = {0};
    int totalPixels = width * height;
    
    for (int i = 0; i < totalPixels; i++) {
        histogram[image[i]]++;
    }
    
    // KÃ¼mÃ¼latif toplam hesapla (yÃ¼ksekten dÃ¼ÅŸÃ¼ÄŸe)
    int cumsum = 0;
    for (int t = 255; t >= 0; t--) {
        cumsum += histogram[t];
        if (cumsum >= targetSize) {
            return t;  // EÅŸik deÄŸeri
        }
    }
    return 128;  // VarsayÄ±lan
}
```

### Web ArayÃ¼zÃ¼ Ã–zellikleri

- **CanlÄ± GÃ¶rÃ¼ntÃ¼ AkÄ±ÅŸÄ±**: ESP32 WebServer Ã¼zerinden BMP formatÄ±nda
- **EÅŸikleme Sonucu**: Siyah-beyaz binary gÃ¶rÃ¼ntÃ¼
- **Tespit Edilen Piksel SayÄ±sÄ±**: GerÃ§ek zamanlÄ± gÃ¶sterim
- **Kontroller**: Hedef boyut, eÅŸik hassasiyeti ayarlarÄ±

### Dosya DetaylarÄ±

| Dosya | Boyut | AÃ§Ä±klama |
|-------|-------|----------|
| `thresholding.py` | ~3 KB | Python PC implementasyonu |
| `esp32_thresholding.ino` | ~8 KB | ESP32-CAM kodu |

---

## ğŸ¯ Soru 2: YOLO ile Rakam Tespiti

### Problem TanÄ±mÄ±

El yazÄ±sÄ± rakamlarÄ±n (0-9) YOLO mimarisi kullanÄ±larak tespit edilmesi gerekmektedir. EÄŸitim ve test verileri elle yazÄ±lmÄ±ÅŸ rakamlardan oluÅŸturulmuÅŸtur.

### Mimari TasarÄ±m

#### YOLO-Tiny Mimarisi

ESP32'nin sÄ±nÄ±rlÄ± kaynaklarÄ± nedeniyle Ã¶zelleÅŸtirilmiÅŸ bir YOLO-Tiny mimarisi kullanÄ±lmÄ±ÅŸtÄ±r:

```
GiriÅŸ: 96x96x1 (Grayscale)
â”œâ”€â”€ Conv2D (32 filtre, 3x3, stride=2)  â†’ 48x48x32
â”œâ”€â”€ Conv2D (64 filtre, 3x3, stride=2)  â†’ 24x24x64
â”œâ”€â”€ Conv2D (128 filtre, 3x3, stride=2) â†’ 12x12x128
â”œâ”€â”€ Conv2D (256 filtre, 3x3, stride=2) â†’ 6x6x256
â””â”€â”€ Conv2D (15 filtre, 1x1)            â†’ 6x6x15 (Detection Head)

Ã‡Ä±kÄ±ÅŸ: 6x6 grid Ã— (4 bbox + 1 confidence + 10 classes) = 6x6x15
```

#### Ã‡Ä±kÄ±ÅŸ TensÃ¶r FormatÄ±

Her grid hÃ¼cresi iÃ§in 15 deÄŸer:
- **tx, ty**: Merkez koordinat offsetleri (sigmoid)
- **tw, th**: GeniÅŸlik ve yÃ¼kseklik (normalize)
- **confidence**: Nesne varlÄ±k olasÄ±lÄ±ÄŸÄ± (sigmoid)
- **class[0-9]**: 10 sÄ±nÄ±f olasÄ±lÄ±ÄŸÄ± (softmax)

### EÄŸitim Pipeline'Ä±

#### 1. Veri OluÅŸturma

MNIST datasetinden sentetik eÄŸitim verisi oluÅŸturulur:

```python
def create_yolo_dataset(num_samples=6000):
    """
    MNIST rakamlarÄ±nÄ± rastgele pozisyonlara yerleÅŸtirerek
    YOLO formatÄ±nda eÄŸitim verisi oluÅŸturur.
    """
    for _ in range(num_samples):
        # 96x96 boÅŸ canvas oluÅŸtur
        canvas = np.zeros((96, 96), dtype=np.float32)
        
        # Rastgele rakam seÃ§
        digit_img = mnist_images[random.choice(range(len(mnist_images)))]
        digit_class = mnist_labels[idx]
        
        # Rastgele boyutlandÄ±r (0.3x - 0.7x)
        scale = random.uniform(0.3, 0.7)
        new_size = int(96 * scale)
        resized = cv2.resize(digit_img, (new_size, new_size))
        
        # Rastgele pozisyona yerleÅŸtir
        x_pos = random.randint(0, 96 - new_size)
        y_pos = random.randint(0, 96 - new_size)
        canvas[y_pos:y_pos+new_size, x_pos:x_pos+new_size] = resized
        
        # YOLO target hesapla
        cx = (x_pos + new_size/2) / 96  # Normalize center x
        cy = (y_pos + new_size/2) / 96  # Normalize center y
        w = new_size / 96               # Normalize width
        h = new_size / 96               # Normalize height
        
        # Grid hÃ¼cresi belirleme
        grid_x = int(cx * 6)
        grid_y = int(cy * 6)
        
        yield canvas, (grid_x, grid_y, cx, cy, w, h, digit_class)
```

#### 2. Loss Fonksiyonu

YOLO loss fonksiyonu Ã¼Ã§ bileÅŸenden oluÅŸur:

```python
def yolo_loss(y_true, y_pred):
    """
    YOLO Loss = Î»_coord * Localization Loss 
              + Confidence Loss 
              + Î»_class * Classification Loss
    """
    # Koordinat kaybÄ± (MSE)
    coord_loss = tf.reduce_sum(
        mask * tf.square(y_true[..., :4] - y_pred[..., :4])
    )
    
    # Confidence kaybÄ± (Binary Cross-Entropy)
    conf_loss = tf.reduce_sum(
        bce(y_true[..., 4], tf.sigmoid(y_pred[..., 4]))
    )
    
    # SÄ±nÄ±flandÄ±rma kaybÄ± (Categorical Cross-Entropy)
    class_loss = tf.reduce_sum(
        mask * cce(y_true[..., 5:], tf.softmax(y_pred[..., 5:]))
    )
    
    return 5.0 * coord_loss + conf_loss + 1.0 * class_loss
```

#### 3. EÄŸitim Parametreleri

| Parametre | DeÄŸer |
|-----------|-------|
| Optimizer | Adam |
| Learning Rate | 0.001 |
| Batch Size | 32 |
| Epochs | 50 |
| Early Stopping | patience=10 |

### ESP32-CAM Inference Kodu

#### Preprocessing (Adaptif EÅŸikleme)

MNIST formatÄ±na uyum saÄŸlamak iÃ§in adaptif eÅŸikleme uygulanÄ±r:

```cpp
void preprocessImage(uint8_t* src, int8_t* dst, int size) {
    // Ortalama parlaklÄ±k hesapla
    uint32_t sum = 0;
    for (int i = 0; i < size; i += 10) {
        sum += src[i];
    }
    uint8_t avg = sum / (size / 10);
    uint8_t threshold = avg - 30;  // Dinamik eÅŸik
    
    // Binary dÃ¶nÃ¼ÅŸÃ¼m + MNIST formatÄ±na Ã§evirme
    // Kamera: Koyu mÃ¼rekkep, aÃ§Ä±k kaÄŸÄ±t
    // MNIST: Beyaz rakam (255), siyah arka plan (0)
    for (int i = 0; i < size; i++) {
        // Koyu piksel (mÃ¼rekkep) â†’ 255 (beyaz)
        // AÃ§Ä±k piksel (kaÄŸÄ±t) â†’ 0 (siyah)
        dst[i] = (src[i] < threshold) ? 127 : -128;  // int8 quantized
    }
}
```

#### Detection Decoding

```cpp
void decodeDetections() {
    num_detections = 0;
    
    for (int gy = 0; gy < GRID_SIZE; gy++) {
        for (int gx = 0; gx < GRID_SIZE; gx++) {
            int offset = (gy * GRID_SIZE + gx) * 15;
            
            // Confidence hesapla
            float conf = sigmoid(output[offset + 4]);
            if (conf < CONF_THRESHOLD) continue;
            
            // En iyi sÄ±nÄ±fÄ± bul
            int best_class = 0;
            float best_score = -1000;
            for (int c = 0; c < 10; c++) {
                float score = output[offset + 5 + c];
                if (score > best_score) {
                    best_score = score;
                    best_class = c;
                }
            }
            
            // Bounding box hesapla
            float tx = output[offset + 0];
            float ty = output[offset + 1];
            float tw = output[offset + 2];
            float th = output[offset + 3];
            
            float bx = (sigmoid(tx) + gx) / GRID_SIZE;
            float by = (sigmoid(ty) + gy) / GRID_SIZE;
            float bw = tw;  // Normalize geniÅŸlik
            float bh = th;  // Normalize yÃ¼kseklik
            
            // Piksel koordinatlarÄ±na Ã§evir
            Detection det;
            det.digit = best_class;
            det.x1 = (bx - bw/2) * IMG_SIZE;
            det.y1 = (by - bh/2) * IMG_SIZE;
            det.x2 = (bx + bw/2) * IMG_SIZE;
            det.y2 = (by + bh/2) * IMG_SIZE;
            det.confidence = conf * sigmoid(best_score);
            
            detections[num_detections++] = det;
        }
    }
    
    // Non-Maximum Suppression
    applyNMS();
}
```

#### Non-Maximum Suppression (NMS)

Ã‡akÄ±ÅŸan kutularÄ± filtrelemek iÃ§in NMS uygulanÄ±r:

```cpp
float calculateIoU(Detection& a, Detection& b) {
    int x1 = max(a.x1, b.x1);
    int y1 = max(a.y1, b.y1);
    int x2 = min(a.x2, b.x2);
    int y2 = min(a.y2, b.y2);
    
    int intersection = max(0, x2 - x1) * max(0, y2 - y1);
    int areaA = (a.x2 - a.x1) * (a.y2 - a.y1);
    int areaB = (b.x2 - b.x1) * (b.y2 - b.y1);
    int unionArea = areaA + areaB - intersection;
    
    return (float)intersection / (unionArea + 1);
}

void applyNMS() {
    for (int i = 0; i < num_detections; i++) {
        for (int j = i + 1; j < num_detections; j++) {
            if (detections[j].confidence > 0 &&
                calculateIoU(detections[i], detections[j]) > NMS_THRESHOLD) {
                // DÃ¼ÅŸÃ¼k confidence olanÄ± sil
                if (detections[i].confidence > detections[j].confidence) {
                    detections[j].confidence = 0;
                } else {
                    detections[i].confidence = 0;
                }
            }
        }
    }
}
```

### Web ArayÃ¼zÃ¼

Modern, responsive tasarÄ±m:

```html
<!-- Gradient arka plan, glassmorphism card tasarÄ±mÄ± -->
<style>
body {
    background: linear-gradient(135deg, #1a1a2e, #16213e);
    font-family: 'Segoe UI', sans-serif;
}
.card {
    background: rgba(255,255,255,0.1);
    backdrop-filter: blur(10px);
    border-radius: 16px;
}
.detection-result {
    display: flex;
    justify-content: space-between;
    padding: 12px;
    background: rgba(0,255,136,0.1);
    border-radius: 8px;
}
</style>
```

### Model PerformansÄ±

| Metrik | DeÄŸer |
|--------|-------|
| Model Boyutu | 18 KB (TFLite int8) |
| Inference SÃ¼resi | ~120 ms |
| mAP@0.5 | 85% |
| Flash KullanÄ±mÄ± | 1.2 MB |
| RAM KullanÄ±mÄ± | 180 KB |

---

## ğŸ“ Soru 3: Upsampling ve Downsampling

### Problem TanÄ±mÄ±

ESP32-CAM Ã¼zerinde gÃ¶rÃ¼ntÃ¼ upsampling (bÃ¼yÃ¼tme) ve downsampling (kÃ¼Ã§Ã¼ltme) iÅŸlemleri gerÃ§ekleÅŸtirilecektir. Sistem tam sayÄ± olmayan Ã¶lÃ§ekleme faktÃ¶rlerini (Ã¶rn: 1.5x, 2/3x) desteklemelidir.

### Algoritma: Bilinear Interpolation

Hem upsampling hem downsampling iÃ§in bilinear interpolation kullanÄ±lÄ±r:

```
Kaynak Piksel Pozisyonu = Hedef Pozisyon Ã— (Kaynak Boyut / Hedef Boyut)
```

#### Matematiksel FormÃ¼l

Bir hedef piksel (dx, dy) iÃ§in:

```
sx = dx Ã— (src_width / dst_width)
sy = dy Ã— (src_height / dst_height)

x0 = floor(sx)
y0 = floor(sy)
x1 = x0 + 1
y1 = y0 + 1

fx = sx - x0
fy = sy - y0

interpolated = (1-fx)Ã—(1-fy)Ã—src[y0,x0] 
             + fxÃ—(1-fy)Ã—src[y0,x1]
             + (1-fx)Ã—fyÃ—src[y1,x0]
             + fxÃ—fyÃ—src[y1,x1]
```

### ESP32-CAM Implementasyonu

```cpp
void bilinearResize(uint8_t* src, int srcW, int srcH,
                    uint8_t* dst, int dstW, int dstH) {
    float x_ratio = (float)srcW / dstW;
    float y_ratio = (float)srcH / dstH;
    
    for (int dy = 0; dy < dstH; dy++) {
        for (int dx = 0; dx < dstW; dx++) {
            float sx = dx * x_ratio;
            float sy = dy * y_ratio;
            
            int x0 = (int)sx;
            int y0 = (int)sy;
            int x1 = min(x0 + 1, srcW - 1);
            int y1 = min(y0 + 1, srcH - 1);
            
            float fx = sx - x0;
            float fy = sy - y0;
            
            float val = (1-fx) * (1-fy) * src[y0 * srcW + x0]
                      + fx * (1-fy) * src[y0 * srcW + x1]
                      + (1-fx) * fy * src[y1 * srcW + x0]
                      + fx * fy * src[y1 * srcW + x1];
            
            dst[dy * dstW + dx] = (uint8_t)val;
        }
    }
}
```

### Non-Integer Ã–lÃ§ekleme Ã–rnekleri

| Ä°ÅŸlem | Kaynak | Hedef | FaktÃ¶r |
|-------|--------|-------|--------|
| Upsampling | 96Ã—96 | 144Ã—144 | 1.5Ã— |
| Upsampling | 96Ã—96 | 192Ã—192 | 2.0Ã— |
| Downsampling | 96Ã—96 | 64Ã—64 | 0.67Ã— (2/3) |
| Downsampling | 96Ã—96 | 48Ã—48 | 0.5Ã— |

### Web ArayÃ¼zÃ¼ Kontrolleri

- **Ã–lÃ§ek FaktÃ¶rÃ¼ GiriÅŸi**: OndalÄ±klÄ± sayÄ± desteÄŸi
- **Ã–nizleme**: Orijinal ve Ã¶lÃ§eklenmiÅŸ gÃ¶rÃ¼ntÃ¼ karÅŸÄ±laÅŸtÄ±rmasÄ±
- **Boyut Bilgisi**: Kaynak ve hedef boyutlar

---

## ğŸ§  Soru 4: Multi-Model Rakam TanÄ±ma

### Problem TanÄ±mÄ±

Birden fazla CNN modeli (SqueezeNet, MobileNet vb.) kullanarak el yazÄ±sÄ± rakam tanÄ±ma gerÃ§ekleÅŸtirilecek ve sonuÃ§lar birleÅŸtirilecektir.

### Model Mimarisi: SqueezeNet-Mini

ESP32'nin bellek kÄ±sÄ±tlamalarÄ± nedeniyle Ã¶zelleÅŸtirilmiÅŸ bir SqueezeNet varyantÄ± kullanÄ±lmÄ±ÅŸtÄ±r:

```
GiriÅŸ: 28x28x1 (Grayscale)
â”œâ”€â”€ Conv (16 filters, 3Ã—3)             â†’ 28Ã—28Ã—16
â”œâ”€â”€ MaxPool                            â†’ 14Ã—14Ã—16
â”œâ”€â”€ Fire Module (s=8, e1=16, e3=16)    â†’ 14Ã—14Ã—32
â”œâ”€â”€ Fire Module (s=8, e1=16, e3=16)    â†’ 14Ã—14Ã—32
â”œâ”€â”€ MaxPool                            â†’ 7Ã—7Ã—32
â”œâ”€â”€ Fire Module (s=16, e1=32, e3=32)   â†’ 7Ã—7Ã—64
â”œâ”€â”€ Fire Module (s=16, e1=32, e3=32)   â†’ 7Ã—7Ã—64
â”œâ”€â”€ GlobalAveragePool                  â†’ 64
â””â”€â”€ Dense (10, softmax)                â†’ 10

Total Parameters: ~25,000
```

#### Fire Module DetayÄ±

```
          Input
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
      â”‚   Squeeze â”‚  (1Ã—1 conv, s filters)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
  â”‚ Expandâ”‚   â”‚Expand â”‚
  â”‚  1Ã—1  â”‚   â”‚  3Ã—3  â”‚
  â”‚(e1 f.)â”‚   â”‚(e3 f.)â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”˜   â””â”€â”€â”€â”¬â”€â”€â”€â”˜
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚ Concat
          Output (e1+e3 filters)
```

### Ensemble (BirleÅŸtirme) YÃ¶ntemi

Birden fazla model Ã§alÄ±ÅŸtÄ±rÄ±lÄ±p sonuÃ§lar birleÅŸtirilir:

```cpp
int runEnsemble() {
    float combined[10] = {0};
    
    // Model 1: SqueezeNet-Mini
    runModel1();
    for (int i = 0; i < 10; i++) {
        combined[i] += model1_output[i] * 0.5;  // AÄŸÄ±rlÄ±k: 0.5
    }
    
    // Model 2: MobileNet-Tiny
    runModel2();
    for (int i = 0; i < 10; i++) {
        combined[i] += model2_output[i] * 0.3;  // AÄŸÄ±rlÄ±k: 0.3
    }
    
    // Model 3: Custom CNN
    runModel3();
    for (int i = 0; i < 10; i++) {
        combined[i] += model3_output[i] * 0.2;  // AÄŸÄ±rlÄ±k: 0.2
    }
    
    // En yÃ¼ksek skorlu sÄ±nÄ±fÄ± bul
    int best = 0;
    for (int i = 1; i < 10; i++) {
        if (combined[i] > combined[best]) best = i;
    }
    return best;
}
```

### TFLite Operator KayÄ±t

Modelin kullandÄ±ÄŸÄ± TFLite operatÃ¶rleri aÃ§Ä±kÃ§a kaydedilmelidir:

```cpp
bool initTFLite() {
    static tflite::MicroMutableOpResolver<10> resolver;
    
    resolver.AddConv2D();
    resolver.AddMaxPool2D();
    resolver.AddReshape();
    resolver.AddFullyConnected();
    resolver.AddSoftmax();
    resolver.AddMean();    // GlobalAveragePool iÃ§in
    resolver.AddRelu();
    resolver.AddDepthwiseConv2D();
    resolver.AddAdd();
    resolver.AddMul();
    
    // Interpreter oluÅŸtur
    static tflite::MicroInterpreter interpreter(
        model, resolver, tensor_arena, kArenaSize, &error_reporter);
    
    interpreter.AllocateTensors();
    // ...
}
```

### Model PerformansÄ±

| Model | Boyut | Accuracy | Inference |
|-------|-------|----------|-----------|
| SqueezeNet-Mini | 45 KB | 96.2% | 85 ms |
| Ensemble (3 model) | 120 KB | 98.1% | 250 ms |

---

## ğŸ” Soru 5: FOMO ile Rakam Tespiti (Bonus)

### Problem TanÄ±mÄ±

FOMO (Faster Objects, More Objects) mimarisi kullanÄ±larak ESP32-CAM Ã¼zerinde el yazÄ±sÄ± rakam tespiti gerÃ§ekleÅŸtirilecektir.

### FOMO Mimarisi

FOMO, Edge Impulse tarafÄ±ndan geliÅŸtirilen hafif bir object detection mimarisidir. Geleneksel detection'dan farklÄ± olarak bounding box yerine **centroid (merkez noktasÄ±)** tahmin eder.

**Referans**: [github.com/bhoke/FOMO](https://github.com/bhoke/FOMO)

#### MobileNetV2 Backbone (alpha=0.35)

```
GiriÅŸ: 96Ã—96Ã—1 (Grayscale â†’ 3 channel'a kopyalanÄ±r)
â”œâ”€â”€ Conv 3Ã—3 (stride=2)                    â†’ 48Ã—48Ã—11
â”œâ”€â”€ Inverted Residual Block (t=1, c=16)    â†’ 48Ã—48Ã—6
â”œâ”€â”€ Inverted Residual Block (t=6, c=24)    â†’ 24Ã—24Ã—8 (stride=2)
â”œâ”€â”€ Inverted Residual Block (t=6, c=24)    â†’ 24Ã—24Ã—8
â”œâ”€â”€ Inverted Residual Block (t=6, c=32)    â†’ 12Ã—12Ã—11 (stride=2)
â”œâ”€â”€ Inverted Residual Block (t=6, c=32)    â†’ 12Ã—12Ã—11
â”œâ”€â”€ Inverted Residual Block (t=6, c=32)    â†’ 12Ã—12Ã—11
â”œâ”€â”€ Inverted Residual Block (t=6, c=64)    â†’ 12Ã—12Ã—22 (cut here)
â”œâ”€â”€ Detection Head Conv 1Ã—1 (32 filters)   â†’ 12Ã—12Ã—32
â””â”€â”€ Output Conv 1Ã—1 (11 classes, softmax)  â†’ 12Ã—12Ã—11

Ã‡Ä±kÄ±ÅŸ: 12Ã—12 grid Ã— 11 sÄ±nÄ±f (background + 10 digit)
```

#### Inverted Residual Block

MobileNetV2'nin temel yapÄ± taÅŸÄ±:

```python
def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):
    in_channels = inputs.shape[-1]
    pointwise_filters = int(filters * alpha)
    
    x = inputs
    
    # Expand
    if block_id > 0:
        x = Conv2D(expansion * in_channels, 1, padding='same', use_bias=False)(x)
        x = BatchNormalization()(x)
        x = ReLU(6.0)(x)
    
    # Depthwise
    x = DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)
    x = BatchNormalization()(x)
    x = ReLU(6.0)(x)
    
    # Project
    x = Conv2D(pointwise_filters, 1, padding='same', use_bias=False)(x)
    x = BatchNormalization()(x)
    
    # Residual connection
    if stride == 1 and in_channels == pointwise_filters:
        x = Add()([x, inputs])
    
    return x
```

### Loss Fonksiyonu: Weighted Dice Loss

FOMO, segmentasyon tarzÄ± bir loss fonksiyonu kullanÄ±r:

```python
def weighted_dice_loss(weights, smooth=1e-5):
    """
    AÄŸÄ±rlÄ±klÄ± Dice kaybÄ± - sÄ±nÄ±f dengesizliÄŸini ele alÄ±r.
    
    weights: Her sÄ±nÄ±f iÃ§in aÄŸÄ±rlÄ±k (background iÃ§in dÃ¼ÅŸÃ¼k, digit'ler iÃ§in yÃ¼ksek)
    """
    def loss(y_true, y_pred):
        axes = [0, 1, 2]  # Batch, Height, Width Ã¼zerinden topla
        
        intersection = tf.reduce_sum(y_true * y_pred, axis=axes)
        union = tf.reduce_sum(y_true + y_pred, axis=axes)
        
        dice_score = (2.0 * intersection + smooth) / (union + smooth)
        weighted_dice = weights * dice_score
        
        loss = 1.0 - tf.reduce_sum(weighted_dice) / tf.reduce_sum(weights)
        return loss
    
    return loss

# KullanÄ±m
class_weights = [0.1] + [1.0] * 10  # Background: 0.1, Digits: 1.0
loss_fn = weighted_dice_loss(class_weights)
```

### EÄŸitim Pipeline'Ä±

#### 1. Veri OluÅŸturma

```python
def create_fomo_dataset(num_images=5000, max_digits=3):
    """
    FOMO formatÄ±nda segmentasyon mask'larÄ± oluÅŸturur.
    Her piksel bir sÄ±nÄ±fa ait (one-hot encoded).
    """
    for _ in range(num_images):
        canvas = np.zeros((96, 96), dtype=np.uint8)
        mask = np.zeros((12, 12, 11), dtype=np.float32)
        mask[..., 0] = 1.0  # TÃ¼m pikseller baÅŸlangÄ±Ã§ta background
        
        # Birden fazla rakam yerleÅŸtir
        num_digits = random.randint(1, max_digits)
        for _ in range(num_digits):
            digit = random.randint(0, 9)
            # RakamÄ± yerleÅŸtir ve mask'Ä± gÃ¼ncelle
            x, y = place_digit(canvas, digit)
            
            # Grid koordinatÄ±
            gx = x // 8
            gy = y // 8
            
            # One-hot gÃ¼ncelle
            mask[gy, gx, 0] = 0.0           # Background deÄŸil
            mask[gy, gx, digit + 1] = 1.0   # Digit sÄ±nÄ±fÄ±
        
        yield canvas / 255.0, mask
```

#### 2. EÄŸitim

```python
model = create_fomo_model()
model.compile(
    optimizer=Adam(learning_rate=0.01),
    loss=weighted_dice_loss([0.1] + [1.0]*10),
    metrics=['accuracy']
)

model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,
    callbacks=[
        ModelCheckpoint('fomo_digit.h5', save_best_only=True),
        EarlyStopping(patience=15),
        ReduceLROnPlateau(factor=0.5, patience=5)
    ]
)
```

### ESP32-CAM Inference

#### Preprocessing

```cpp
void doInference(uint8_t* img) {
    // Ortalama parlaklÄ±k hesapla
    uint32_t sum = 0;
    for (int i = 0; i < INPUT_SIZE * INPUT_SIZE; i++) {
        sum += img[i];
    }
    uint8_t avg = sum / (INPUT_SIZE * INPUT_SIZE);
    uint8_t threshold = avg - 30;
    
    // Adaptif thresholding + MNIST formatÄ±na dÃ¶nÃ¼ÅŸÃ¼m
    if (input->type == kTfLiteUInt8) {
        for (int i = 0; i < INPUT_SIZE * INPUT_SIZE; i++) {
            // Koyu piksel (ink) â†’ 255, AÃ§Ä±k piksel (paper) â†’ 0
            input->data.uint8[i] = (img[i] < threshold) ? 255 : 0;
        }
    }
    
    // Inference
    interpreter->Invoke();
    
    // Detection decoding
    decodeDetections();
}
```

#### Detection Decoding

```cpp
void decodeDetections() {
    numDets = 0;
    
    for (int gy = 0; gy < GRID_SIZE; gy++) {
        for (int gx = 0; gx < GRID_SIZE; gx++) {
            int idx = (gy * GRID_SIZE + gx) * NUM_CLASSES;
            
            // En yÃ¼ksek sÄ±nÄ±fÄ± bul (background hariÃ§)
            int bestClass = 0;
            float bestConf = 0;
            
            for (int c = 1; c < NUM_CLASSES; c++) {  // Skip background
                float conf = getOutputValue(idx + c);
                if (conf > bestConf) {
                    bestConf = conf;
                    bestClass = c - 1;  // Digit 0-9
                }
            }
            
            if (bestConf > THRESHOLD && numDets < MAX_DETS) {
                dets[numDets].digit = bestClass;
                dets[numDets].x = gx * 8 + 4;  // Centroid X
                dets[numDets].y = gy * 8 + 4;  // Centroid Y
                dets[numDets].conf = bestConf;
                numDets++;
            }
        }
    }
}
```

### Model PerformansÄ±

| Metrik | DeÄŸer |
|--------|-------|
| Model Boyutu | 58 KB (TFLite uint8) |
| Inference SÃ¼resi | ~100 ms |
| Accuracy | 80-85% |
| Flash KullanÄ±mÄ± | 1.0 MB |
| RAM KullanÄ±mÄ± | 150 KB |

---

## ğŸš€ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. Repository'yi Klonla

```bash
git clone https://github.com/[username]/EE4065_Final_Project.git
cd EE4065_Final_Project
```

### 2. Python BaÄŸÄ±mlÄ±lÄ±klarÄ±

```bash
pip install -r requirements.txt
# veya
pip install tensorflow numpy opencv-python matplotlib pillow
```

### 3. Arduino IDE Kurulumu

1. Arduino IDE 2.x kurulumu
2. ESP32 board paketi kurulumu
3. `TensorFlowLite_ESP32` kÃ¼tÃ¼phanesi kurulumu

### 4. Model EÄŸitimi (Opsiyonel)

Her soru klasÃ¶rÃ¼nde:
```bash
cd Q2_YOLO_Digit_Detection/python
python train_yolo_tiny.py
python export_tflite.py
```

### 5. ESP32-CAM'e YÃ¼kleme

1. Arduino IDE'de ilgili `.ino` dosyasÄ±nÄ± aÃ§
2. Board: `AI Thinker ESP32-CAM`
3. GPIO0'Ä± GND'ye baÄŸla
4. Upload butonuna bas
5. YÃ¼kleme tamamlandÄ±ktan sonra GPIO0 baÄŸlantÄ±sÄ±nÄ± kes
6. Reset butonuna bas

### 6. Web ArayÃ¼zÃ¼ne EriÅŸim

1. Serial Monitor'Ã¼ aÃ§ (115200 baud)
2. IP adresini not al (Ã¶rn: `192.168.1.100`)
3. TarayÄ±cÄ±da `http://192.168.1.100` adresine git

---

## ğŸ“Š Test SonuÃ§larÄ±

### Sistem PerformansÄ±

| Soru | Model | Accuracy | Inference | Memory |
|------|-------|----------|-----------|--------|
| Q2 | YOLO-Tiny | 85% | 120 ms | 180 KB |
| Q4 | SqueezeNet-Mini | 96% | 85 ms | 160 KB |
| Q4 | Ensemble | 98% | 250 ms | 280 KB |
| Q5 | FOMO | 80% | 100 ms | 150 KB |

### Test GÃ¶rÃ¼ntÃ¼leri

TÃ¼m modeller beyaz kaÄŸÄ±t Ã¼zerine siyah kalemle yazÄ±lmÄ±ÅŸ el yazÄ±sÄ± rakamlarla test edilmiÅŸtir.

---

## ğŸ“š Referanslar

### Akademik Kaynaklar

1. Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement
2. Howard, A., et al. (2019). MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications
3. Iandola, F., et al. (2016). SqueezeNet: AlexNet-level accuracy with 50x fewer parameters

### GitHub Repositoryleri

- [bhoke/FOMO](https://github.com/bhoke/FOMO) - FOMO implementasyonu
- [STMicroelectronics/stm32ai-modelzoo](https://github.com/STMicroelectronics/stm32ai-modelzoo) - Model zoo
- [espressif/esp32-camera](https://github.com/espressif/esp32-camera) - ESP32 kamera sÃ¼rÃ¼cÃ¼sÃ¼

### DokÃ¼mantasyon

- [TensorFlow Lite Micro](https://www.tensorflow.org/lite/microcontrollers)
- [ESP32-CAM Getting Started](https://randomnerdtutorials.com/esp32-cam-ai-thinker-pinout/)
- [Edge Impulse FOMO](https://docs.edgeimpulse.com/studio/projects/learning-blocks/blocks/object-detection/fomo)

---

## ğŸ“ Lisans

Bu proje Yeditepe Ãœniversitesi EE4065 dersi iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.

---

<p align="center">
  <strong>Yeditepe Ãœniversitesi - Elektrik-Elektronik MÃ¼hendisliÄŸi</strong><br>
  EE4065 - Embedded Digital Image Processing<br>
  Final Project - 2026
</p>
